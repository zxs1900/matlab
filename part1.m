%% Invididual Project Part 1
% Detailed below
%% Part1 (1)

% random normal distribution error and x
N = 100; %100 observations
rng('default'); %store value
rng(1);
error1 = normrnd(0,1,[N,1]); %mean 0, standard deviation 1
rng(2);
x = normrnd(0,1,[1,N]);

%beta
beta1 = 10;
beta2 = 2.5;

%y
y1 = beta1 + beta2 * x' + error1;
%%
% The normal series of error has been generated by using MATLAB built-in 
% function ‘normrnd()’ with mean of 0 and standard deviation of 1. 
% To ensure the value generated does not change as we run the file, 
% ‘rng()’ has been used to lock the results of random generation. 
% The independent variable x has been generated in the same way. 
% As the value of beta has been given where beta1 equal to 10 and beta2 
% equal to 2.5, the dependent variable y can be calculated by the linear 
% model: y = beta1 + beta2  * x + error.
%%
%figure of error
figure()
histogram(error1,'BinWidth',0.3);
%%
% The error is generally normally distributed indicated by the graph.

%%
%figure of X and Y
figure()
plot(x,y1,'LineStyle','none','Marker','.');
%%
% According to the graph, X and Y are positively related.


%% Part1 (2)

%linear regression
mylm2 = mlr(x,y1);
%
% <<mylm2.png>>
%
%%
% As shown above,the OLS regression provides unbiased estimated results 
% which minimize the sum of squared residual. It shows that the estimate 
% beta1 and beta 2 equal to 9.9376 and 2.3769 respectively. The p-value for
% both beta1 and beta2 are 0 ,which indicates that estimtate beta1 and
% beta2 are significant in all level. R squared is relatively The residual 
% and error has been plotted in the same graph to shown the difference 
% between them. For the same period, the residual is slightly differ 
% from the error which randomly generated by MATLAB.

%%
%estimate y
yhat2 = mylm2.result(1) + mylm2.result(2) * x';

%residual
residual2 = y1 - yhat2;

%%
%figure of error and residual
figure()
plot(1:N,error1,1:N,residual2,'LineStyle','none','Marker','.');
legend('error','residual');

%%
% Compare the pair of error and residual, there are considerable distance
% between them. Still, the regression results indicates that the residual
% is rational consider the R squared and p-value.

%%
% The residual is generally normally distributed.


%% Part1 (4)

%random normal distribution error and x with 1000 observation

N = 1000;
rng('default');
rng(1);
error4 = normrnd(0,1,[N,1]); %mean 0, standard deviation 1
rng(2);
x4 = normrnd(0,1,[1,N]);

%beta
beta1 = 10;
beta2 = 2.5;

%y
y4 = beta1 + beta2 * x4' + error4;

%%
%figure of error
figure()
histogram(error4);
%%
% The error term is normally distributed as same as in question 2.

%%
%figure of X and Y
figure()
plot(x4,y4,'LineStyle','none','Marker','.');



%ols regression
mylm4 = mlr(x4,y4);
mylm4

%%
% The accuracy of one linear regression model is related to the size of 
% observation according to the central limits theorem . It is obvious 
% that as we increase the observation from 100 to 1000, the estimated 
% beta1 and beta2 become more accurate. In the meanwhile, the p-value for 
% both coefficient are 0 as well as our estimated beta1 and beta2 are 
% significant at all level.

%%
%estimate y
yhat4 = mylm4.result(1) + mylm4.result(2) * x4';

%residual
residual4 = y4 - yhat4;

%%
%figure of error and residual
figure()
plot(1:N,error4,1:N,residual4,'LineStyle','none','Marker','.');
legend('error','residual');
%%
% The difference between error and residual still exists.


%% Part1 (5)

%2 parts of errors
N = 100;
rng(3);
halfA = normrnd(0,1,[N/2,1]); %mean 0, standard deviation1
rng(4);
halfB = normrnd(0,2,[N/2,1]); %mean 0 ,standard deviation2
error5 = [halfA;halfB];

%y
y5 = beta1 + beta2 * x' + error5;



%ols regression
mylm5 = mlr(x,y5);
mylm5

%%
% The estimated beta1 approached to its true value, however, the estimated
% beta2 was away from its true value. They both significant accourding to
% p-value. When we split the error to 2 parts with different volatility,
% it has higher influence on the estimated value.

%estimate y
yhat5 = mylm5.result(1) + mylm5.result(2) * x';

%residual
residual5 = y5 - yhat5;


%%
%figure of error and residual
figure()
plot(1:N,error5,1:N,residual5,'LineStyle','none','Marker','.');
legend('error','residual');
%%
% It is obvious that error and residual spread more widely in the right
% hand side, which due to the 2 parts of error terms with differenct
% standard deviation.
%%
%plot of residual
figure();
histogram(residual5);


%% Part1 (6)
n=100; %sample size
N=1000; %simulations
monteBeta1=zeros(1,N); %Preallocation
monteBeta2=zeros(1,N);

for i=1:N
    beta1=5;
    beta2=0.7;
    error = normrnd(0,1,[n,1]);
    x = normrnd(0,1,[1,n]);
    y = beta1 + beta2 * x' + error;
   
    
    mylm6=fitlm(x,y);
    
    
    monte6Beta1(i)=mylm6.Coefficients.Estimate(1);
    monte6Beta2(i)=mylm6.Coefficients.Estimate(2);
    
    
end

%%
%figure of beta1
figure()
subplot(1,2,1)
histogram(monte6Beta1,'BinWidth',0.01);
title('beta1')
subplot(1,2,2)
histogram(monte6Beta2,'BinWidth',0.01);
title('beta2')


%%
% As we defined tht E(beta) = beta, we would conclude that the estimated
% beta is unbiased as shown in the graph.


%% Part1 (7)
n=100; %sample size
N=1000; %simulations
monteBeta2=zeros(1,N);



for i=1:N
    
    beta2=0.7;
    error = normrnd(0,1,[n,1]);
    x = normrnd(0,1,[1,n]);
    y = beta2 * x' + error;
    
    mylm7=fitlm(x,y,'Intercept',false);
    
    monte7Beta2(i)=mylm7.Coefficients.Estimate;
    
end

%%
%figure of beta2
figure()
histogram(monte7Beta2,'BinWidth',0.01);
title('beta2')

%%
%%%
% As we defined tht E(beta) = beta, we would conclude that the estimated
% beta is unbiased as shown in the graph. There is no influence on the
% result whether we exclude cosntant and beta1, as variables should be
% independent to each other.


%% Part1 (9)

n=100; %sample size
N=1000; %simulations
monteBeta1=zeros(1,N);
monteBeta2=zeros(1,N);
monteBeta3=zeros(1,N);

% x with 15% correlated z
for i=1:N
    
    beta1=5;
    beta2=0.7;
    beta3=5;
    rho1 = 0.15;
    
    
    error = normrnd(0,1,[n,1]);
    x = normrnd(0,1,[1,n]);
    z1 = x*rho1 + normrnd(0,1,[1,n]);
    y = beta1 + beta2 * x' + beta3 * z1'+ error;
    m = [x;z1];

    
    mylm8=fitlm(m',y);
    monte8Beta1(i)=mylm8.Coefficients.Estimate(1);
    monte8Beta2(i)=mylm8.Coefficients.Estimate(2);
    monte8Beta3(i)=mylm8.Coefficients.Estimate(3);
end

%%
%figure of beta1
figure()
subplot(2,2,1)
histogram(monte8Beta1,'BinWidth',0.01);
title('beta1')
subplot(2,2,2)
histogram(monte8Beta2,'BinWidth',0.01);
title('beta2')
subplot(2,2,3)
histogram(monte8Beta3,'BinWidth',0.01);
title('beta3')

%%
% The graph above illustated that with additional variable z with weak
% correlation with x, the unbiasedness property still been satisfied.

%%



% x with 90% correlated z
for i=1:N
    
    beta1=5;
    beta2=0.7;
    beta3=5;
    rho2 = 0.9;
    
    
    error = normrnd(0,1,[n,1]);
    x = normrnd(0,1,[1,n]);
    z2 = x*rho2 + normrnd(0,1,[1,n]);
    y = beta1 + beta2 * x' + beta3 * z2' + error;
    m = [x;z2];
    
    mylm9=fitlm(m',y);
    monte9Beta1(i)=mylm9.Coefficients.Estimate(1);
    monte9Beta2(i)=mylm9.Coefficients.Estimate(2);
    monte9Beta3(i)=mylm9.Coefficients.Estimate(3);
end

%%
%figure of beta1
figure()
subplot(2,2,1)
histogram(monte9Beta1,'BinWidth',0.01);
title('beta1')
subplot(2,2,2)
histogram(monte9Beta2,'BinWidth',0.01);
title('beta2')
subplot(2,2,3)
histogram(monte9Beta3,'BinWidth',0.01);
title('beta3')

%%
% As there is no significant sign that the estimated beta is away from its
% expected value, we would like to conclude that they all satisfy the
% unbiasedness.


%% Part1 (10)
%2 parts of errors
N = 100;
rng(3);
halfA = normrnd(0,1,[N/2,1]); %mean 0, standard deviation1
rng(4);
halfB = normrnd(0,2,[N/2,1]); %mean 0 ,standard deviation2
error5 = [halfA;halfB];

%y
y5 = beta1 + beta2 * x' + error5;



%ols regression
mylm5 = mlr(x,y5);

%estimate y
yhat5 = mylm5.result(1) + mylm5.result(2) * x';

%residual
residual5 = y5 - yhat5;

%%
%qqplot
figure();
normalityResidual = qqplot(residual5);
%%
% Q-Qplot has been used to test the normality of the residual. The graph
% above of the residual shows that it does not conform very well to the
% normal distribution, as we have multiple outliers in both upper-right and
% lower-left coner.

%%
%serial correlated
figure;
autocorr(residual5);
%%
% The built-in function of 'autocorr()' has been used to test the
% serial correlation. As shown above ,it indicates that overall they are
% correlated within an acceptable leve, however, for lags 5, 9, 12, 14 have
% larger correlation.


%%
%heteroskedastik
h = archtest(residual5);
h

%%
% As the result of arch test equal to 0, where we fail to reject the
% hypothesis of no arch effect. To conclude, the residual is not
% heteroskedastik.

%% Part1 (11)
% To resolve the issues addressed in question 10, the GLS would be used.
% For heteroskedasticity and correlation, it is considerable to transform
% the model appropriately so the error of the transformed model can conform
% the manner or equal variance and no correlation.


%% Part1 (12)
N = 100;

indexDaily = readtable('SP500NASDAQ.xls');

x12 = indexDaily.SP500(1:100);
x12 = x12';

y12 = beta1 + beta2 * x12' + error1;

mylm12 = mlr(x12,y12);
mylm12

%%
% The regression results varied a lot compared with question 2 and 3. the
% estimated beta1 and beta2 have large difference to its true value. The
% p-value for beta1 indicates that estimated beta1 is not significant but
% estimated beta2 does. The fitness increased as it almost become 1.

%estimate y
yhat12 = mylm12.result(1) + mylm12.result(2) * x12';

%residual
residual12 = y12 - yhat12;
%%
%figure of X and Y
figure();
plot(x12,y12,'LineStyle','none','Marker','.');

%%
%figure of error and residual
figure()
plot(1:N,error1,1:N,residual12,'LineStyle','none','Marker','.');
legend('error','residual');

%% Part1 (3)

function mytable = mlr(x,y)

%Estimate beta
beta2 = sum((x-mean(x))*(y-mean(y)))/sum((x-mean(x)).^2);
beta1 = mean(y)-beta2*mean(x);
betaHat = [beta1;beta2];

%standard error
n = length(y);
yhat = betaHat(1) + betaHat(2)*x';
RMSE = sqrt(sum((y-yhat).^2)/(n -2));
beta1SE = sqrt(((RMSE.^2)*(n.^-1)*sum(x.^2))/sum((x-mean(x)).^2));
beta2SE = RMSE/sqrt(sum((x-mean(x)).^2));
stdError = [beta1SE;beta2SE];

%t statistic
tSta = [betaHat(1)/stdError(1);betaHat(2)/stdError(2)];


%p value
beta1P =  1-tcdf(abs(tSta(1)),n-2);
beta2P =  1-tcdf(abs(tSta(2)),n-2);
pVal= [2*beta1P;2*beta2P];



%confidence interval
percentile = tinv([0.025 0.975],n-2);
beta1Con = betaHat(1) + stdError(1) * percentile;
beta2Con = betaHat(2) + stdError(2) * percentile;
conIn = [beta1Con beta2Con];


%R squared
yhat = betaHat(1) + betaHat(2) * x';
rSqrd = 1 - (sum((y-yhat).^2)/sum((y-mean(y)).^2));

%adjusted R squared
adjR = 1 - ((1-rSqrd) * (n-1) / (n-2));

%f statistic
fSta = (rSqrd/1)/((1-rSqrd)/(n-2));


result = [betaHat;stdError;tSta;pVal;conIn';rSqrd;adjR;fSta];
mytable = table(result,'RowNames',{'beta1';'beta2';'SE beta1';'SE beta2';...
    'tSta beta1';'tSta beta2';'pValue beta1';'pValue beta2';...
    'confidence upper beta1';'confidence lower beta1';...
    'confidence upper beta2';'confidence lower beta2';'R2';'adjR2';'fSta'});

end



